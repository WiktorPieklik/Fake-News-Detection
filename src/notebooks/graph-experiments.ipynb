{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports & constants"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from os.path import abspath\n",
    "from typing import Tuple, List\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import DistilBertTokenizerFast, TFDistilBertModel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "transformer_checkpoint = 'distilbert-base-uncased'\n",
    "dataset_path = Path(abspath('.')).parent.parent / \"raw\" / \"dataset.csv\"\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "MAX_LENGTH = 512\n",
    "EPOCHS = 10\n",
    "GRAPH_DIM = 5\n",
    "\n",
    "TRAIN_SPLIT = .8\n",
    "VAL_SPLIT = .1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(dataset_path, lineterminator='\\n')\n",
    "dataset.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data imputation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset.description = dataset.description.fillna(\"\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data split (train, val, test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_len = int(dataset.shape[0] * TRAIN_SPLIT)\n",
    "val_len = int(train_len * VAL_SPLIT)\n",
    "train_len -= val_len\n",
    "test_len = dataset.shape[0] - train_len - val_len\n",
    "print(f\"Train: {train_len}, Val: {val_len} & Test: {test_len}\")\n",
    "\n",
    "# train\n",
    "X_train = dataset.iloc[:train_len, :-1].values\n",
    "y_train = dataset.iloc[:train_len, -1].values\n",
    "\n",
    "#val\n",
    "X_val = dataset.iloc[train_len:train_len + val_len, :-1].values\n",
    "y_val = dataset.iloc[train_len:train_len + val_len, -1].values\n",
    "\n",
    "#test\n",
    "X_test = dataset.iloc[train_len + val_len:, :-1].values\n",
    "y_test = dataset.iloc[train_len + val_len:, -1].values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Divide data into graph & nlp"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def divide_dataset(dataset: np.ndarray) -> Tuple[tf.constant, np.ndarray]:\n",
    "    \"\"\" Return (graph_X, nlp_X) \"\"\"\n",
    "\n",
    "    bool_encoder = LabelEncoder()\n",
    "    graph_X = dataset[:, [1, 2, 3, 4, 8]]\n",
    "    nlp_X = dataset[:, [5, 6, 7]]\n",
    "    graph_X[:, 1] = bool_encoder.fit_transform(graph_X[:, 1])\n",
    "\n",
    "    return tf.constant(graph_X, dtype='int32'), nlp_X\n",
    "\n",
    "def prepare_nlp_ds(nlp_dataset: np.ndarray) -> List[str]:\n",
    "    dataset = []\n",
    "    for row in nlp_dataset:\n",
    "        dataset.append(row[0]+ \" \" + row[2])\n",
    "\n",
    "    return dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "graph_train, nlp_train = divide_dataset(X_train)\n",
    "graph_val, nlp_val = divide_dataset(X_val)\n",
    "graph_test, nlp_test = divide_dataset(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "graph_train.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Batch encode"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def batch_encode(tokenizer: DistilBertTokenizerFast, nlp_dataset: np.ndarray):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    for i in range(0, len(nlp_dataset), BATCH_SIZE):\n",
    "        batch = prepare_nlp_ds(nlp_dataset[i: i + BATCH_SIZE])\n",
    "        tokens = tokenizer(\n",
    "            batch,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=False,\n",
    "        )\n",
    "        input_ids.extend(tokens[\"input_ids\"])\n",
    "        attention_mask.extend(tokens[\"attention_mask\"])\n",
    "\n",
    "    return tf.convert_to_tensor(input_ids), tf.convert_to_tensor(attention_mask)\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(transformer_checkpoint)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nlp_train_ids, nlp_train_attention = batch_encode(tokenizer, nlp_train)\n",
    "nlp_val_ids, nlp_val_attention = batch_encode(tokenizer, nlp_val)\n",
    "nlp_test_ids, nlp_test_attention = batch_encode(tokenizer, nlp_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "distilBERT = TFDistilBertModel.from_pretrained(transformer_checkpoint)\n",
    "for layer in distilBERT.layers:\n",
    "    layer.trainable = False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_model(transformer: TFDistilBertModel) -> tf.keras.Model:\n",
    "    input_ids_layer = tf.keras.layers.Input(shape=(MAX_LENGTH,), name='input_ids', dtype='int32')\n",
    "    input_attention_layer = tf.keras.layers.Input(shape=(MAX_LENGTH,), name='input_attention', dtype='int32')\n",
    "    input_graph_layer = tf.keras.layers.Input(shape=(GRAPH_DIM,), name='input_graph_stats', dtype='int32')\n",
    "\n",
    "    features_unbatched = transformer([input_ids_layer, input_attention_layer])[0]\n",
    "    cls_token = features_unbatched[:, 0, :]\n",
    "    x = tf.keras.layers.Dense(5, activation='relu')(input_graph_layer)\n",
    "    x = tf.keras.layers.concatenate([x, cls_token])\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=[input_ids_layer, input_attention_layer, input_graph_layer], outputs=output)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = build_model(distilBERT)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(),  # due to some issues with Apple M1\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tensorflow dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices(((nlp_train_ids, nlp_train_attention, graph_train), y_train))\n",
    "val_ds = tf.data.Dataset.from_tensor_slices(((nlp_val_ids, nlp_val_attention, graph_val), y_val))\n",
    "\n",
    "train_ds = train_ds.cache().batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.cache().batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    history = model.fit(\n",
    "        x=train_ds,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=val_ds\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
